
_Not the actual notes, but notes about the notes._

keywords: access, conversation, basic rules, visibility

Hi, I'm here to talk about a tool I built for testing web-based services
called "Gabbi". My name is Chris Dent and I'm currently working for Red
Hat as part of their OpenStack group.

I've been developing tools somewhere in the vicinity of the web for
quite a long time. I'm in the school of thought that thinks the reason
the web is successful is not just because it is open and standards-based
but because that openness allows and encourages access from all sort of
agents, human and machine. That access is like a conversation: as long
as the things talking are following the basic rules of the language then
anyone or anything that speaks the language can participate.

Tests, too, are a form of conversation. Depending on how they are used
they can confirm that everything is still speaking the same language and
working correctly or help the right language -- the one that lots of
different things can use -- emerge.

I've been working on OpenStack for about a year and a half. It's a cool
and exciting project creating very useful stuff but it sometimes feels
like it has been built by people who hate Python and hate HTTP. The
systems are complex and inscrutable, the learning curve for new
developers is extremely steep and the costs associated with
experimenting and exploring are very high.

In other situations where I've been faced with such confusion I've been
able to use the tests associated with the project as a way of
understanding what's going on. This hasn't worked too well in OpenStack
because many of the tests are in deeply nested class hierarchies that
obscure any sense of what's going on. For the sake of avoiding
repetition, API interactions are hidden behind arbitrary clients that
mask HTTP interactions and error handling. In general it is just too
hard to see what's going on.

All this frustration made me itch, so I scratched. I decided I needed to
create a tool that made it easy to evaluate APIs that was clean, clear
and focused on the HTTP interactions and could be equally useful for
exploring and experimenting with existing APIs and doing test driven
development of new APIs.

I called the result "gabbi" and started using it to test a few OpenStack
projects. It looks a bit like this. To me, at least, the gabbi version
of this test is a good deal more readable. The upper example is more
about Python than HTTP. Python is a great language but that's not really
what we're trying to test here. From just the visible Python would you
be able to reconstruct the URL being used in the request?

{{demonotes.txt}}

Those are just a few of the features in Gabbi. We've seen some of
these but there are more:

* It's possible to access environment variables, perhaps containing
  authentication information for headers.
* Asynchronous resources can be polled a set number of times with a
  delay until an expected result happens or the loop expires. When the
  loop expires the most recent error is reported as the test failure.
* Individual tests can be marked to be skipped or xfailed. This has
  proven very useful. An xfailing test can show the correct behavior
  and be annotated with a reference to a bug report.
* Gabbi uses the load_tests protocol from unittest to generate tests
  that can be handled by many Python testrunners, such as testtools or
  subunit. Each individual file is treated as a custom TestSuite that
  is run in order. If, from the testrunner, you select that only one
  test in a particular file is to be run, all tests prior to that one
  in the suite are run.
* Each file may choose to establish a set of fixtures prior to the
  TestSuite running. These can, for example, establish a persistence
  layer or some sample data.
* If you're developing a test suite for an app that may be mounted in
  different place on a server, Gabbi can deal with this by using a
  prefix. The tests remain the same, the prefix is injected when
  constructing URLs.
* It is possible to mix URLs that have no host and URLs with a host in
  the same YAML file. If no host is present the default defined in the
  runner is used. This can be used to create fully API driven
  integration tests: create a resource over on server A, poll until a
  related resource is brought into existence on server B.
* And finally there's no need to have a running web server at all. If
  the app being tested is a WSGI app, then it can be tested directly
  using wsgi-intercept.

Gabbi is about 9 months old. In that time I've been able to identify
some interesting patterns of use. One important aspect of gabbi is that
it is so easy to write tests that it can be quite fun to use. One of the
most effective ways to use it is to play dumb and just throw random
stuff at the API being tested and see what happens. A sort of human
driven fuzz testing. This tends to reveal lots of bugs, is a fantastic
learning aid and shows holes in the grammar of the API. [If time
consider missing collection URI example.]

More often than not when you're testing you're going to need some kind
of configuration fixture that will make sure that the application knows
where its persistence layer is and what middleware is in place.

One of the biggest things learned from observing people using the tool
is that it can be very tempting to use gabbi too much, either in
evaluating too much in one single test or trying to evaluate, by way
of secondary behaviors, aspects of the system that are beyond the API.
While sometimes this may be useful or required, making a habit of it
results in unreadable tests and then we are back where we started in a
pre-gabbi era.

That's it, thanks very much. Here are some links that are related to
Gabbi. Note that gabbi-demo is the server I used in today's demo. It is
alsoa tutorial on how to do test driven development with Gabbi. If you
read the git commit log in reverse it describes how to make a simple
object API with gabbi tests.

Questions?
